\clearpage

Before writing this section, I sometimes came across expressions such as
$p(x) = A \dd{x}$ or $p(x) = A' m(\dd{x})$ in defining exponential family.
It relates to the fundamental concept of measure and a density in measure theory.
When I see $\dd{x}$ in evaluating integral, e.g. $f(x) = \int_\Rf x \dd{x} = x^2$,
it usually refers to the Lebesgue measure.
In the case of $m(\dd{x})$, it generally emphasize that you are integrating
with respect to a general measure $m$.

We skipped defining the carrier density and its measure for now.


\begin{itemize}
	\item Correct carrying density
	\item Check if cumulant function can be differentiated infinitely
\end{itemize}

\section{Exponential Family}

The exponential family is a collection of parametric models with very elegant properties.
Peter and Hao utilised it to arrive some nice results about
the optimality of \E-variables, mainly relying on the duality.
We called a model belonging to the exponential family
if the underlying distributions can be written below.


\begin{define}[Exponential Family]
	\begin{align}\label{eq:exp=family}
		p_\beta(U)
		:= & \exp\left(\beta^T t(U) - \psi(\beta)\right) r(U) \\
		=  & \frac{1}{Z(\beta)} \exp(\beta^T X) r(U).
	\end{align}
\end{define}

\begin{itemize}
	\item $U$: \emph{random variable}
	\item $X=t(U)$: \emph{sufficient statistics}
	\item $\beta$: \emph{canonical parameter}
	\item $\psi(\beta)$: \emph{cumulant function}
	\item $r(y)$: the \emph{carrying density}
	\item $Z(\beta)$: the \emph{partition function}, $\psi(\beta)=\log Z(\beta)$
\end{itemize}

\subsection{Basic Properties}

Cumulant generating function $\psi(\beta)$: The first and second \emph{cumulant} is mean and variance.
$\psi(\beta)$ is differentiable infinitely.

\begin{define}[Minimal]
	We say an exponential family representation is \emph{minimal}.
	If not minimal, there is $t_{k}$ that can be represented
	by the linear combination of $t_{j}$ for $j\not=k$, requiring 1 less parameter.
\end{define}

We usually assume minimal if not mentioned.

\begin{define}[Canonical Parameter Space]
	Canonical parameter space $\Bt$ is the set of parameter $\beta$
	where the following integration is finite
	\begin{align*}
		\Bt := \left\{\beta: \int_{\Uc} \exp(\beta^T t(U)) r(U) <\infty \right\}.
	\end{align*}

%	$m(\dd{u})$ will either be Lebesgue measures or counting measures for discrete cases.
%	In the first case, `$m(\dd{u})$' reduces to `$\dd{u}$',
%	which can be handled using standard multivariable calculus.
%	While in the latter, the above integral can be
%	written as a summation.
\end{define}

\begin{remark}
	Every exponential family has a canonical parameterization with carrier density $p_0$
	such that the canonical parameter space contains the origin, i.e. $\bm{0} \in \Bt$.
	(Section 18.4.3 in MDL)
\end{remark}

\begin{proof}
	If $\bm{0} \not\in \Bt$ or $r(U)$ is not a carrier density, we can pick any $\beta_0 \in \Bt$
	and set
	\begin{align*}
		r'(U) = \frac{r(U)\exp(\beta t(U))}{\sum_U r(U)\exp(\beta t(U))}
	\end{align*}
	
	Now the family corresponding to $r'$ with parameter $\beta'=\beta-\beta_0$ is identical
	to the original family containing $\bm{0}$.
\end{proof}

\begin{define}[Regular Exponential Family]
	If the canonical parameter space of exponential families are nonempty open set,
	we call such families \emph{regular}.
\end{define}

\begin{define}[Full Exponential Family]
	The family is called \emph{full} if the dimension of $\beta$ equals the dimension of $\Bt$.
\end{define}

\begin{define}[Order]
	The order of an exponential family is the minimal dimension of $t(u)$ such that we can express
	the family using \refEq{eq:exp=family}.
\end{define}

\begin{define}[Minimal Exponential Family]
	An exponential family is referred to as \emph{minimal} if:
	a) there are no linear constraints among the components of the parameter vector $\beta$;
	b) there are no linear constraints among the components of the sufficient statistic $t(u)$
	(in the latter case, with probability one under the measure $m$).
\end{define}

\begin{example}[Non-minimal distribution]
	The simplest would be a multinomial distribution with parameter $()$.
	The PMF can be written as bit
	\begin{align*}
		s
	\end{align*}
\end{example}

We can reparameterize the probability distribution even in the case of minimal distribution\todo{add example}.
Given a (?arbitrary) set $\Theta$ and a mapping $\Phi: \Theta \to \Bt$,
we consider the densities with canonical parameters replaced by $\Phi(\theta)$
\begin{align*}
	p_\theta(u) := \exp(\Phi(\theta)^T t(u) - \phi(\Psi(\theta))).
\end{align*}
Here $\Phi$ is a one-to-one mapping whose image is all of $\Bt$.

If $\Phi$'s image is a strict subset of $\Bt$ ($\Phi(\Theta) \subset \Bt$),
it is then OK to reparameterize on that subset.
If it can not be reducible,
we refer this exponential family as curved.

We are also interested in cases in which the image of $\psi$ is a strict subset of N.
If this subset is a linear subset, then it is possible to transform the representation into an
exponential family on that subset. When the representation is not reducible in this way, we
refer to the exponential family as a curved exponential family.

\begin{define}[Curved Exponential Family]
	TBH
	% Φ(θ):
	% There are some important examples in which the density (pmf) has the basic Exponential family
	% form f (x |θ) = e
	% k
	% i=1 ηi (θ)Ti (X)−ψ(θ)h(x), but the assumption that the dimensions of Θ, and that
	% of the range space of (η1(θ),· · · , ηk (θ)) are the same is violated. More precisely, the dimension of
	% Θ is some positive integer q strictly less than k.
\end{define}

\begin{example}[Normal distribution with variance equalling to mean]
	Let $u \in \Rf \sim \Nc(\mu, \mu^2)$, $\mu \not=0$. The density of $u$ is
	\begin{align*}
		p_\mu(u)
		 & = \abs{\mu}^{-1}(2\pi)^{-1/2} \exp(-\frac{1}{2}(\frac{u-\mu}{\mu})^2)                     \\
		 & = (2\pi)^{-1/2} \exp(-\frac{u^2}{2\mu^2} + \frac{u}{\mu} - \frac{1}{2} + \log(\abs{\mu})) \\
		 & = \exp(\beta^T t(u) - \psi(\beta)).
	\end{align*}

	The $\beta^T = (-1/2\mu^2, 1/\mu)$,
	the sufficient statistics $t(u) = (u^2, u)$.
	The dimension of the sufficient statistic is more than the dimension of $\beta$ for curved exp. family.
\end{example}




I have found some other versions of the same statement regarding minimal exponential family:
\begin{itemize}
	\item A minimal exponential family is where the $t(u)$ are linearly independent
	\item A minimal exponential family is one where representation reaches the \emph{order}
\end{itemize}

All share the same statement regarding the linear dependency of sufficient statistics $t(u)$,
only Michael I. Jordan's Chapter 8 stated on $\beta$.
I wonder if linear dependency in $\beta$ implies linear dependency in $t(u)$, i.e.
\begin{align*}
	a^T \beta = C \; \Leftrightarrow \; b^Tt(u) = C'
\end{align*}

He also claimed that non-minimal families can always be reduced to minimal families
via a suitable transformation and reparameterization.

If an exponential family is not minimal, it is
called \emph{overcomplete}. Both minimal and overcomplete representations are useful



\subsection{Mean-value parameterization}

We 


\begin{define}[Matching pair]
	Let $\Pc, \Qc$ be exponential family for random variable $U$.
	We can they are \emph{matching pair} if 
	\begin{itemize}
		\item they are mutually absolutely continuous wrt each other
		\item they have the same sufficient statistics $X$
		\item their mean-value space, $\Mt_q\subseteq\Mt_p$
		\item for every matching canonical parameterisation of $\Pc$ and $\Qc$, 
				we have $\Bt_p\subseteq\Bt_q$
	\end{itemize}
\end{define}



\subsection{Simple Case}
\begin{itemize}
	\item Why I think this is the carrier probability with some measure $v$ $p_{\bm{\mu}^*} = p_{\bm{0}, \bm{\mu}^*} \quad q_{\bm{0},\bm{\mu}^*} = q = q_{\bm{\mu}^*}$:

	      It is rather simple when you write down $\bm{\beta}=\bm{0}$
	      \begin{align*}
		      p_{\bm{0};\bm{\mu}^*}(u) & :=
		      \frac{1}{Z_p(\bm{0};\bm{\mu}^*)}\exp(\bm{0}^T t(u))\cdot p_{\bm{\mu}^*}(u)                               \\
		                               & = \frac{p_{\bm{\mu}^*}(u)}{Z_p(\bm{0};\bm{\mu}^*)}                            \\
		                               & = \frac{p_{\bm{\mu}^*}(u)}{\int\exp(\bm{0}^T t(u)) p_{\bm{\mu}^*}(u)\dd{\nu}}
		      = p_{\bm{\mu}^*}(u)
	      \end{align*}

	\item A naive question follows: what is the distribution $p_{\bm{\mu}^*}(u)$?
	      Is it in the mean-value parameterization or canonical form?
\end{itemize}

\subsection{General Case}

\begin{itemize}
	\item Why Hao explicitly denote the mean of the carrier density in \cite{grunwaldSafeTesting2024}:
\end{itemize}

We first discuss the Gaussian location family where the mean $\bm{\mu}^*\in\Mt_p=\Rf^d$.
The null distribution will be family with $\bm{\mu}^*$ and a positive semidefinite covariance matrix $\Sigma_p$
and the alternative $\Qc=\{Q\}$ would be Gaussian distribution with the same $\bm{mu}^*$ and
a covariance matrix $\Sigma_p \not= \Sigma_q$.

The notation $D_{\textsc{Gauss}}$ in Eq. (3.2.1) in \cite{haoEvaluesAnytimevalidInference2025} is
just the KL divergence. Here $X=U=(X_1,\dots,X_d)$ is the $d$-dimensional random vector with
distribution $p$ or $q$

\begin{align*}
	D_{\textsc{Gauss}}(B) & = \KL{P}{Q}                                                                                                             \\
	                      & := \int_{\Uc} p(X) \log \frac{p(X)}{q(X)}
	= \ERWi{p}{\log\frac{p(X)}{q(X)}}                                                                                                               \\
	                      & = \ERWi{p}{\log\frac{(2\pi)^{-d/2} \det(\Sigma_p)^{-1/2}\exp(-\frac{1}{2}(X-\bm{\mu}^*)^T \Sigma_p^{-1}(X-\bm{\mu}^*))}
	{(2\pi)^{-d/2} \det(\Sigma_q)^{-1/2}\exp(-\frac{1}{2}(X-\bm{\mu}^*)^\top \Sigma_q^{-1}(X-\bm{\mu}^*))}}                                         \\
	                      & = \ERWi{p}{\log\frac{\det(\Sigma_q)^{-1/2} \exp(\Sigma_p^{-1})}{\det(\Sigma_q)^{-1/2}\exp(\Sigma_q^{-1})}} \qq{Wrong!}
\end{align*}

For more details, check \url{https://statproofbook.github.io/P/mvn-kl.html} for cases where the means are different.

Why Eq. (3.4.2) holds? The part I don't understand is that
why putting a prior will equal to $p_{\bm{\mu}^*}$

\begin{align*}
	S_{\textsc{cond}} := \frac{q_{W_1}(U^{(n)} \;|\; Z)}{p_{W_0}(U^{(n)} \;|\; Z)}
	\stackrel{?}{=} \frac{q_{\bm{\mu}^*}(U^{(n)} \;|\; Z)}{p_{\bm{\mu}^*}(U^{(n)} \;|\; Z)}
\end{align*}

The key is just simple derivation with the fact that $\hat{X}_{|n}:=\sum X_i/n \sim \Nc(n\bm{\mu}^*,n\Sigma_q)$.
Setting $Z=\hat{X}_{|n}/\sqrt{n} \sim \Nc(\sqrt{n}\bm{\mu}^*,\Sigma_q)$, we have
\begin{align*}
	q_{W_1}(X^{(n)} \;\mid\; Z) & := \frac{q_{W_1}(X^{(n)}, Z)}{q_{W_1}(Z)} = \frac{q_{W_1}(X^{(n)})}{q_{W_1}(Z)}
	\stackrel{iid}{=} \frac{\prod^n_{i=1}q_{W_1}(X_i)}{q_{W_1}(Z)}                                                                                  \\
	                            & = \frac{\prod(2\pi)^{-d/2}(\det \Sigma_q)^{-1} \exp(-1/2 (X_i-\bm{\mu}^*)^\top \Sigma_q^{-1} (X_i - \bm{\mu}^*))}
	{(2\pi)^{-d/2}(\det \Sigma_q)^{-1} \exp(-1/2(Z -\sqrt{n} \bm{\mu}^*)^\top \Sigma_q^{-1} (Z -\sqrt{n} \bm{\mu}^*))}                              \\
	                            & = \frac{\exp(\sum_{i=1}^n X_i^\top\Sigma_q^{-1}X_i - 2 \sqrt{n}{\bm{\mu}^*}^\top\Sigma_q^{-1}Z
		+ n {\bm{\mu}^*}^\top \Sigma_q^{-1}\bm{\mu}^*)}
	{\exp(Z^\top\Sigma_q^{-1}Z - 2 \sqrt{n}{\bm{\mu}^*}^\top\Sigma_q^{-1}Z
	+ n {\bm{\mu}^*}^\top \Sigma_q^{-1}\bm{\mu}^*)}                                                                                                 \\
	                            & = \exp(\sum_{i=1}^n (X_i - Z)^\top \Sigma_q^{-1} (X_i - Z))
	% {}\qq{ignoring non-exp part}
\end{align*}

The equality follows due to the joint distribution $X^{(n)}$ and $Z$ is simply $X^{(n)}$.

For UI case, I don't see any sign of splitting? It looks like just ML prequential settings

What makes the \E-process-ness? th

Why the $S_{\textsc{seq}} = S_{\textsc{seq,rip}}$ in Thm. 2?

\subsection{One-dimensional special case}

\begin{align*}
	 & \Gc := \{g_{\eta}(y), \eta \in A,  \in \Yc \}, \quad A \text{ and } \Yc \in \Rf^p \\
	 & g_{\eta}(y) := \exp\left(\eta y - \psi(\eta)\right) g_0(y) m(\dd{y})
\end{align*}

\begin{remark}[Carrying density in p.3 efron]
	Any $g_{\eta_0}(y) \in \Gc$ could be the carrier density and the members of $\Gc$
	are absolutely continuos with respect to each other,
	i.e. their null measure sets agree.
\end{remark}

\begin{remark}
	\textit{Cumulant generating function} for $\psi(\eta)$ originates from
	the old techniques for finding expectations, variances and higher-order moments.
\end{remark}

\subsection{Moment Relationships}

We can differentiate $\exp(\psi(\eta)) = \int_{\Yc} \exp(\eta y) g_0(y)m(\dd{y})$

\begin{align*}
	\dot{\psi}(\eta) \exp(\psi(\eta)) & = \int_{\Yc} y\exp(\eta y) g_0(y) m(\dd{y})              \\
	\ERWi{g_0}{y} = \dot{\psi}(\eta)  & = \int_{\Yc} y\exp(\eta y - \psi(\eta)) g_0(y) m(\dd{y})
\end{align*}

Differentiating $\exp(\psi(\eta))$ twice gives:
\begin{align*}
	(\ddot{\psi}(\eta) + \dot{\psi}(\eta)) \exp(\psi(\eta))
	                            & = \int_{\Yc} y^2 \exp(\eta y) g_0(y) m(\dd{y})                   \\
	\VAR{y} = \ddot{\psi}(\eta) & = \int_{\Yc} (y^2 - y)\exp(\eta y - \psi(\eta)) g_0(y) m(\dd{y})
\end{align*}


Efron's book (Chap. 2)
\begin{align*}
	 & \Gc := \{g_{\eta}(y), \eta \in A, y \in \Yc \}, \quad A \text{ and } \Yc \in \Rf^p \\
	 & g_{\eta}(y) := \exp\left(\eta^T y - \psi(\eta)\right) g_0(y) m(\dd{y})
\end{align*}
\begin{itemize}
	\item $g_0(y)$: \textit{carrying density} w.r.t. some \textit{carrying measure} $m(\dd{y})$ on $\Yc$
	\item $A$ is the \textit{canonical parameter space}:
	\item $\psi(\eta)$ is the \textit{normalizing function} or \textit{cumulant generating function} (CGF)
\end{itemize}
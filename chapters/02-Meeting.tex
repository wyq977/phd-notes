\newpage

\section{Meetings}

\subsection*{26.09.2025}

There are quite a lot of things that I need to check after this. 
Nick presented on conformal prediction where my take-away about is that 
one could relate the

Another theme is still the contingency table. 
There is something strange about the NML sequential 
in which the estimation is on the entire sequence.

Another problem that I need to write down is that for a given prior $\pi$
and Bayesian updating on the blocks, is the multiplication equal to the 
global one.

Simpler proof: 
1) write something in closed form, e.g. exponential
2) compare it to the proof of seq-RIPr and RIPr in general case.
3) but I think the seq-COND is not there yet?

I need to refresh on NP-lemma, product measure, outer measure, push-forward measure
and etc.

I also wanna do a step-by-step guide on plotting and figure in a standalone repo.

\subsection*{05.09.2025}

\subsubsection*{What I did this week}

In terms of coding, what I did is transform the big genotype $G$ and phenotype $P$ data
into a small subset. There are mainly two reasons:
1) 455k SNPs exceeds our scope of multiple testing, and we really need a handful
of interesting targets
2) the 44 SNPs in the small dataset are not ``significant''

1) Run Fisher exact test with \texttt{PLINK --assoc fisher}
2) Rank them by p-value, FDR control ($0.05$) 
3) Obtain about 125 SNPs with very small $p$

Noted that this is for the 4688 sample points where we are simply testing 
with Fisher exact test for association. 
Furthermore, I synthesise sequential dataset by shuffling into blocks 
of 50 whose ratio of case between control is close to global.

\begin{table}[h]
\centering
\caption{rs17375018: Fisher's Exact Test p-value of $9.08 \times 10^{-9}$.}
\begin{tabular}{lrr}
\toprule
 & SNP+ ($a$) & SNP- ($b$)\\
\midrule
case ($1$) & 1779 & 866 \\
ctrl ($0$) & 4089 & 2626 \\
\bottomrule
\end{tabular}
\end{table}

One might think that in a global significant SNPs, the pseudo-sequential
data blocks would likely yield significant results. 
However, we observed multiple $p_i = 1$, e.g., on tables as below.
\begin{table}[h]
\centering
\begin{tabular}{lrr}
\toprule
 & $a$ & $b$\\
\midrule
$1$ & 46 & 29 \\
$0$ & 16 & 9 \\
\bottomrule
\end{tabular}
\end{table}

Furthermore, we are trying to calculate the probability of 
$p_{\mathrm{fisher}}=1$ for the pseudo blocks.
Start with the simple one and use union bound. 

\begin{align*}
	\condPR{\frac{n'_{a1}}{n'_{a0}} \approx \frac{n_1}{n_0}}{
	n'_a, n'_b, \frac{n_a}{n_b} \approx \frac{n_a'}{n_b'}
	} 
\end{align*}

For now, we consider the cases, $\approx$ is $=$ avoiding situations
where non-integers appears and ceiling/flooring functions are applied.

\begin{itemize}
	\item Check out this \url{https://scholar.google.com/citations?user=UnrY-40AAAAJ&hl=en}
	\item \url{https://en.wikipedia.org/wiki/Boole%27s_inequality}
\end{itemize}

\subsection*{29.08.2025}

I mainly talked about the gap between the test supermartingale and \E-process.
I was confused but Peter corrected me, saying even though you can find a 
supermartingale upper bounding the \E-process.

In the simple null case, they are the same. However, in the composite settings,
it is not, for each $P\in\Pc$, there is a test supermartingale but
there is not a single one for all $P\in\Pc$.
In fact, it's very easy to give examples where a random process is \E-process 
but not supermartingale.
% TODO: add counter-example

He mentioned examples from UI, which is very straightforward to show it's \E 
and Alex also said something about the canonical filtrations, by Ramdas, in
the t-test.

Seb talks about Gaussian channelling, a paper on 2004 with 3rd derivative 
and information theory. Interesting, worth a look.

\subsection*{25.08.2025}
Practical goal:
\begin{itemize}
	\item Compare speed of conditional E in BiasedUrn and R: about 3x performance, not worth it to import
	\item Add save E into the package, not done yet but the styles are already similar to the safestats
	      pacage
\end{itemize}

This week's goal theory-wise
\begin{itemize}
	\item Just write down again the KL for Gaussian family for repetition.
	\item Restate the connection between integral and sequential product of UI:
	      haven't found the literature yet
	\item Try to re-state the simple and anti-simple case
	\item What is the seq-RIPr and seq-COND?
	\item Why do we need to have a general KL measure in general paper
	\item Read carefully how the maximum is found in the log-optimal paper
	\item Reproduce with safestats packages for the
	\item UMP in math. stat. course lecture notes.
	\item I wanna write down $\dd{\nu}$ and $\dd{X}$ and all that.
\end{itemize}

\emph{The difference between the simple and anti-simple case} In short, the simple case
is where $\Sigma_q-\Sigma_p$ is negative which mean we can find a RIPr via a prior (or a element)
of $P$.

\subsection*{15.08.2025} Sebastian did a great presentation regarding testing
quantile given filtrations. I would formulate here and also add a picture.

The question in mind is to test whether data $X$ is from a hypothesis $\Hc$ that
is non-parametric:
\begin{align*}
	\Hc = \left\{P \in \Pc(X) \mid \ERWi{p}{\phi_i(X)=0}, \i=1,2,\dots,d\right\} \\
	\text{where } \Pc(X) \text{ is all distribution on } X.
\end{align*}

The simplest instance where testing $X$ with the same mean $\mu$ where
$\phi_1(X)=X-\mu$.
Larson has proven that the `optimal' \E-variable must be in the form:
\begin{align*}
	S := 1 +\sum_{i=1}^d \lambda_i \phi_i(X)
\end{align*}

Sebastian is mainly interested in the cases if there is any gain in \E-variables
compared to a coraser filtrations.
Imagine we have conditioned on the original data $X_i, i=1,2,\dots$, you would think
that the hypothesis case constructed as below:
\begin{align}
	\Hc  & = \left\{P \in \Pc(X) \mid \condERW{Y_2}{X_1} = \alpha \right\} \\
	\Hc' & = \left\{P \in \Pc(X) \mid \condERW{Y_2}{Y_1} = \alpha \right\}
\end{align}
where $Y_i = 1_{X_i \leq q}$. He showed that both $\Hc$ and $\Hc'$ are convex
and $\Hc$'s closed convex hull is not the same, otherwise it would be kind of pointless.

\adjustimage{max size={0.5\textheight}{\textwidth},keepaspectratio}{fig/2025-08-15.jpeg}

\subsection*{14.08.2025}

This is a short discussion regarding UI, specifically about the difference
between prequential and integral representation.

Supposed iid data $\Dc=\{X_1,X_2,\dots,X_n\}$ and we denote
the data up to time $i$ by $X^{(i)}=\{X_1,X_2\dots,X_i\}$.

One way to instantiate UI by
\begin{align*}
	\frac{\prod_{i=1}^n P_{\tilde{\theta}_{alt\mid i-1}}(X_i)}{P_{\hat{\theta}_0}(X^{(n)})}
\end{align*}
where $\tilde{\theta}_{alt\mid i-1}$ is any estimator in alternative based on
the first $i-1$ data points $X^{(i-1)}$
and $\hat{\theta}_0=\argmax_{\theta\in\Theta_0} \prod_i p_{\theta}(X_i)$
is the MLE estimator under the null.
See more details in section 7 of UI paper.

Notice that the sequence of the data $\Dc=\{X_1,\dots X_n\}$ really matters.
Imagine you obtian $\Dc'$ with a rearranged sequence and thus slightly
different $\tilde{\theta}$ and hence slightly different value at the
denominator.

Revisit factorization of probability, it's also called chain rule or general product rule:
\begin{align*}
	\PR{X_1,X_2,\dots,X_n} = \PR{X_1}\condPR{X_2}{X_1}\cdots \condPR{X_n}{X_1,X_2,\dots,X_{n-1}} \\
	= \prod_{i=1}^n \condPR{X_i}{X^{(i-1)}}
\end{align*}

\begin{question}
	Below is a mixture of all nonnegative test martingale/e-process?
\end{question}

\begin{align*}
	\frac{\int p_{\theta}(X^{(n)}) w(\theta) \dd{\theta}}{P_{\hat{\theta}_{\mathrm{null}}}(X^{(n)})}
\end{align*}

\adjustimage{max size={0.5\textheight}{\textwidth},keepaspectratio}{fig/2025-08-14.jpeg}

\subsection*{25.07.2025} I only briefly went over the conformal prediction and fisher's noncentral hypergeometric distribution.
There were some discussions on what the conformal prediction is.

Imagine you have a classifier for images (dogs, cat, etc.) and is trained via $N$ datasets.
For the next prediction, we need something to quantify the uncententy to say that
\begin{quote}
	The label for $X_{N+1}$ I gave being $X$ (here could be any label), has
\end{quote}

Peter gave a algorithmic explanantion where the predicted labels are gaven for each label,
then run through against the previous training datasets. Rank them, cut off the tailing $\alpha$
percent then we can say we are confident about our prediction with $1-\alpha$.

However, this is awfully similar to the permutation test, by Sebastian.
Yeah it does look a lot like just ranking the prediction and give a p-value.

Alexander also suggested using Gaussian for conformal prediction might be too confusing
as the parameter and the prediction kind of just are the same thing.
Maybe try Poisson example where parameter is in real number while the prediction is in $\Zf$.

What I do not follow is the output of the classifier is a weighted matrix over all the labels.
What is the ranking being done over? Is it

\subsection*{21.05.2025}

Papers discussed: PNAS and general case

For the simple case (the Gaussian location family in 3.2.1 in \cite{haoEvaluesAnytimevalidInference2025}),
my interpretation is as followed:
If negative semidefnite $\Sigma_q-\Sigma_p$, then we reduced to the simple case where the RIPr \E-variable is not only
in $\conv(\Pc)$ (the convex hull of the null dist.) but rather a element of $\Pc$ itself.

Otherwise, we can find a prior on $\Pc$ with sharp variance $\Nc(\bm{\mu}^*, (\Sigma_q-\Sigma_p)/n)$.
The results in turn suggests that we cast a sharp prior on $\Pc$ with the same mean.
However, extra argument about the alternative $Q$, I think we move to the distributions from $Q$
that shares the same sufficient statistics.
This is still the part where I am having some doubts.
I am still sure is that what does it mean to operate on a `enlarged' or `modified'
alternative even if the alternative $\Qc=\{Q\}$ being just Gaussian with same mean and a different variance.

Another point is in the Anti-Simple case, then $S_{Q,\textsc{seq,rip}}^{(n)} =1$.
First of all, the superscript $^{(n)}$ says it is considering a sequential settings
but $X^{(i)}$ is \emph{singular} following either null dist. or alternative dist.
This $S=1$ breaks simply when we are considering two $X$ following the same distribution,
we then test whether $X' = (X, Y) \iid P \in \Pc$ or alternative.

\textbf{Data spilting in UI} \quad In a sense, the spilting is done sequentially or done across $n$
data points in contrast to the $D_0$ $D_1$ in the original paper. Now we are talking about the UI in (3.2.14)
I think! The classical on would be $L = L_0(\hat{\theta}_1)/L_0(\hat{\theta}_0)$

\begin{align*}
	S_{\breve{\bm{\mu}},\textsc{UI}}^{(n)} =
	\frac{\prod_{i=1}^n q_{\breve{\bm{\mu}}|_{i-1}(U_{(i)})}}
	{p_{\breve{\bm{\mu}}|_n(U^{(n)})}}
\end{align*}

We have a regular ML estimator likelihood in the denominator after looking at the whole sequence $n$.
For the nominator, we have a product of the ML likelihood for each time $i$.
In the end, we have effectively losing just $U_{(1)}$ in calculating likelihood.

\textbf{Beyond NP} \quad We discussed about the similarities among the loss function in the ERM scheme
and in this paper. The ERM loss is always with respect to some data (empirical) while
here we are concerning about a data-dependent loss?

The problem or rather common difficulty with p-value in NP paradigm is that
why do we report p-value at all?
What does it mean for a small p-value when $p\ll\alpha$?

Peter proposed a alternative as in we should report E-value instead.
I think also the notation of seeing $\alpha$ as the error probability is somewhat challenged.

\textbf{Inspiration} \quad In vaccination study, we see a extremely small p-value on null
but we are not allowed to make \emph{new} decision without setting up new hypothesises and
new studies. In other words, if we looked at a promising data \emph{post-hoc},
there is really not much we can do based on the original data.
But with \E-value, he argued that new decision can be based/conditioned on the data.

Is $l$ in Eq. 1 \cite{grunwaldNeymanPearsonEvalues2024} just a new $\alpha$?

In this loss function $L(\kappa, a)$, $\kappa$ is the true state of nature that we don't know?
We just assumed if the data is coming from null or alternative.

In all, I think the main idea is to change from Type-I error safe to Type-I risk safe.
I think section 2's main take home message is that any maximally compatible decision rule must
be a \E-variable

This is cited from Micheal I Jordan's notes:
\url{https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf}


\clearpage
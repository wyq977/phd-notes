\newpage

\section{Meetings} 

\subsection{Nonnegative martingales and E-process}



\subsection*{25.08.2025} 
Practical goal:
\begin{itemize}
    \item Compare speed of conditional E in BiasedUrn and R: about 3x performance, not worth it to import
    \item Add save E into the package: forked from
\end{itemize}


This week's goal theory-wise
\begin{itemize}
    \item Just write down again the KL for Gaussian family for repeatition.
    \item Restate the connection between integral and sequential product of UI:
    haven't found the literature yet
    \item Try to re-state the simple and anti-simple case
    \item What is the seq-RIPr and seq-COND?
    \item Why do we need to have a general KL measure in general paper
    \item Read carefully how the maximum is found in the log-optimal paper 
    \item Reproduce with safestats packages for the 
    \item UMP in math. stat. course lecture notes.
    \item I wanna write down $\dd{\nu}$ and $\dd{X}$ and all that.
\end{itemize}

\emph{The difference between the simple and anti-simple case} In short, the simple case
is where $\Sigma_q-\Sigma_p$ is negative which mean we can find a RIPr via a prior (or a element)
of $P$.

\subsection*{15.08.2025} Sebastian did a great presentation regarding testing 
quantile given filtrations. I would formulate here and also add a picture.

The question in mind is to test whether data $X$ is from a hypothesis $\Hc$ that
is non-parametric:
\begin{align*}
    \Hc = \left\{P \in \Pc(X) \mid \ERWi{p}{\phi_i(X)=0}, \i=1,2,\dots,d\right\} \\
    \text{where } \Pc(X) \text{ is all distribution on } X.
\end{align*}

The simplest instance where testing $X$ with the same mean $\mu$ where
$\phi_1(X)=X-\mu$.
Larson has proven that the `optimal' \E-variable must be in the form:
\begin{align*}
    S := 1 +\sum_{i=1}^d \lambda_i \phi_i(X)
\end{align*}

Sebastian is mainly interested in the cases if there is any gain in \E-variables
compared to a coraser filtrations. 
Imagine we have conditioned on the original data $X_i, i=1,2,\dots$, you would think
that the hypothesis case constructed as below:
\begin{align}
    \Hc  &= \left\{P \in \Pc(X) \mid \condERW{Y_2}{X_1} = \alpha \right\} \\
    \Hc' &= \left\{P \in \Pc(X) \mid \condERW{Y_2}{Y_1} = \alpha \right\}
\end{align}
where $Y_i = 1_{X_i \leq q}$. He showed that both $\Hc$ and $\Hc'$ are convex
and $\Hc$'s closed convex hull is not the same, otherwise it would be kind of pointless.

\adjustimage{max size={0.5\textheight}{\textwidth},keepaspectratio}{fig/2025-08-15.jpeg}

\subsection*{14.08.2025}

This is a short discussion regarding UI, specifically about the difference
between prequential and integral representation.

Supposed iid data $\Dc=\{X_1,X_2,\dots,X_n\}$ and we denote 
the data up to time $i$ by $X^{(i)}=\{X_1,X_2\dots,X_i\}$.

One way to instantiate UI by
\begin{align*}
\frac{\prod_{i=1}^n P_{\tilde{\theta}_{alt\mid i-1}}(X_i)}{P_{\hat{\theta}_0}(X^{(n)})}
\end{align*}
where $\tilde{\theta}_{alt\mid i-1}$ is any estimator in alternative based on 
the first $i-1$ data points $X^{(i-1)}$
and $\hat{\theta}_0=\argmax_{\theta\in\Theta_0} \prod_i p_{\theta}(X_i)$ 
is the MLE estimator under the null. 
See more details in section 7 of UI paper.

Notice that the sequence of the data $\Dc=\{X_1,\dots X_n\}$ really matters.
Imagine you obtian $\Dc'$ with a rearranged sequence and thus slightly 
different $\tilde{\theta}$ and hence slightly different value at the 
denominator.

Revisit factorization of probability, it's also called chain rule or general product rule:
\begin{align*}
    \PR{X_1,X_2,\dots,X_n} = \PR{X_1}\condPR{X_2}{X_1}\cdots \condPR{X_n}{X_1,X_2,\dots,X_{n-1}} \\
    = \prod_{i=1}^n \condPR{X_i}{X^{(i-1)}}
\end{align*}

\begin{question}
    Below is a mixture of all nonnegative test martingale/e-process?
\end{question}

\begin{align*}
\frac{\int p_{\theta}(X^{(n)}) w(\theta) \dd{\theta}}{P_{\hat{\theta}_{\mathrm{null}}}(X^{(n)})}
\end{align*}

\adjustimage{max size={0.5\textheight}{\textwidth},keepaspectratio}{fig/2025-08-14.jpeg}

\subsection*{25.07.2025} I only briefly went over the conformal prediction and fisher's noncentral hypergeometric distribution.
There were some discussions on what the conformal prediction is.

Imagine you have a classifier for images (dogs, cat, etc.) and is trained via $N$ datasets. 
For the next prediction, we need something to quantify the uncententy to say that
\begin{quote}
    The label for $X_{N+1}$ I gave being $X$ (here could be any label), has
\end{quote}

Peter gave a algorithmic explanantion where the predicted labels are gaven for each label, 
then run through against the previous training datasets. Rank them, cut off the tailing $\alpha$
percent then we can say we are confident about our prediction with $1-\alpha$.

However, this is awfully similar to the permutation test, by Sebastian. 
Yeah it does look a lot like just ranking the prediction and give a p-value.

Alexander also suggested using Gaussian for conformal prediction might be too confusing 
as the parameter and the prediction kind of just are the same thing. 
Maybe try Poisson example where parameter is in real number while the prediction is in $\Zf$.

What I do not follow is the output of the classifier is a weighted matrix over all the labels.
What is the ranking being done over? Is it 


\section{TODO}
This is just a playground for me! No need to be too nicely formatted.

\subsection{Questions}

\begin{itemize}
    \item What is the regularity condition in universal inference condition?
    \item Why do we want to have data-dependent significant level $\alpha$
    \item What is the difference between \p-process and \E-process and nonnegative martingale? 
    \item What is a adapted sequence of random sets, random variables
    \item Wald's sequential likelihood ratio test
    \item What is Radon-Nikodym derivative 
    \item Try to explain post-hod and ad-hoc in plain English
    \item What would happen if you just multiply \E-variables together? Shouldn't you consider the 
    \item What is the evidence in testing? Is likelihood ratio the best one we have for evidence 
\end{itemize}

\begin{itemize}
    \item What is the difference between carrying measure and carrying measure?
    \item Write down the lemma/fact that each element in a regular exponential family is continuous 
    with respect to each other and could be used as carrier. Is it a one-to-one mapping?
    \item Mean, Variance, Fisher information
    \item Mean-value Parameterization Convexity of mean-value spaces and canonical spaces
    \item Duality
    \item Loewner ordering
\end{itemize}

\subsection{Filtrations and sigma-fields}

\begin{define}[Filtration]
    Let $(\Omega, \Ac, P)$ be a probability space. An increasing sequence $\Fc_n$ of sub-$\sigma$ algebras of $\Ac$ 
    (i.e. $\Fc_0 \subseteq \Fc_1 \subseteq \cdots \Fc_n \subseteq \cdots \subseteq \Ac$) is called a filtration.
\end{define}

The sub-$\sigma$-algebra is just the sigma algebra of the $X_0, X_1, X_n$.

What is the difference between the stopping time and random times that can take possibly infinity.

Filtration sigma-field at time $t$, filtration is a n increasing sequence of sigma-fields.

What is the lim sup here? $A_t$ is an adapted sequences of events in some filtered probability space.
$$
A_\infty := \limsup_{t\to \infty} A_t := \bigcap_{t\in\Nf}\bigcup_{s\geq t}A_s
$$

\begin{define}[Absolute continuous]
    Let $p, q$ be two probability distributions. $p$ is called \emph{absolutely continuous} with respect to $q$ if the 

    More explicitly, this reads ($p \ll q$) that 
    \begin{align*}
        q(A) = 0 \quad \Rightarrow \quad p(A) = 0,  \quad \text{for any } A \in \Fc_t \text{ and } t \in \Nf.
    \end{align*}
\end{define}

\subsection{All the inequalities}

Hoeffding, Bernstein, McDiarmid, Talagrand's

Evidence lower bound

line crossing inequality
average treatment effect
mixture adaptive design

\subsection{Papers, talks, textbook, and more topics}

Clarke Barron 1990 1994 about
\begin{itemize}
    \item Stein's Methods
    \item Ito's process
    \item Levy's process
    \item Gaussian process
\end{itemize}

\subsection{Basic Concepts}

\begin{define}[Admissibility]
    In \E-process, a process is inadmissible 
    if there exists a process that is strictly better than it at certain time points.
\end{define}

Calibration

Dominates

UMP

Understanding the Fourier transform in terms of transform
	Placing a restriction on the Fourier transform == smaller function space
Minimax setup

Integration / Measure

Topology crash course

The topologist Stephen Smale stunned the mathematical community in
1958 [Sma58] when he proved it was possible to turn the sphere inside out
without introducing any creases. Several ways to do this are beautifully
illustrated in video recordings [Max77, LMM94, SFL98].

Tower properties





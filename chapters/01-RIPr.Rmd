\newcommand{\conv}[1]{\mathrm{conv}(#1)}


# Reverse Inverse Projections

::: {#kl .definition name="Kullback-Leibler divergence"}
$$
D(P \Vert Q) = 
$$
:::

::: {#rip .definition name="RIP"}
11
:::

Here we assume a family $\mathcal{Q}$ (singleton or composite) of parametric family.

And we assume the distribution of null hypothesis is well supported $P$ and
begin with single element for simplicity.

## Li's Algorithm

Originally developed by @liEstimationMixtureModels1999, the following algorithm
is written based on the notes from @haoEvalues$k$sampleTests2024.

The gist is to obtain the RIPr in a greedy manner where the KL divergence betwee
distribtion $Q$ onto the convex hull of a set of distributions $\mathcal{Q}$ 
(composite null) is minimized. It is assumed that the KL divergence between $Q$ 
and any distribution $Q \in \mathcal{Q}$ is finite.

Here the step is denoted by $k = 1, 2, \dots$.

1.  **Step 1**, $k=1$, $Q_{(1)}$ is chosen where 
$$
    Q_{(1)} = \arg\min_{Q \in \mathcal{Q}}D(P \Vert Q)
$$ 

It can also be written as when viewing parameter space: $$
    Q_{(1)} = Q_{\theta_{(1)}} = \arg\min_{\theta \in \Theta}D(P \Vert Q_{\theta})
    $$
2.  **Step** $2, 3, \dots$, **do**
  
  $$
  Q_{(m)} := \alpha Q_{(m-1)} + (1 - \alpha) Q' 
  $$
  where $\alpha \in (0,1)$ and $Q'$ is a single element of $\mathcal{Q}$ or $\conv{\mathcal{Q}}$? (NOT SURE).
  
Here, the distribution $Q'$ and $\alpha$ is chosen (coupled) such that the divergence $D(P \Vert \alpha Q_{(m-1)} + (1 - \alpha) Q')$ is minimized. The 
minimizer need not be unique (*I think*).

**Regularity condition on $\mathcal{Q}$**  

> Li's algorithm is apprantly greedy with high fluctaction in initial steps.
> Aditionally, this task is computationally expensive and it is not clear of the convexity.

### Is the returned mixture in the convex hull of $\mathcal{Q}$

*Correction*:
The returned $Q_{(m)}$ is in the convex hull by definition. The first step 
returned a single element in $\mathcal{Q}$ with the smallest KL divergence.
Iteratively, the linear combination is still in the convex hull, e.g.

\begin{align*}
Q_{(2)} &= \alpha_1 \cdot Q_{(1)} + (1 - \alpha_1)\cdot Q_{(1)}' \\
Q_{(2)} &= \alpha_2 \cdot Q_{(2)} + (1 - \alpha_2)\cdot Q_{(2)}' \\
&= \alpha_2 \cdot \alpha_1 \cdot Q_{(1)} + 
\alpha_2 \cdot (1 - \alpha_1) \cdot Q_{(1)}' + 
(1 - \alpha_2)\cdot Q_{(2)}' \\
\end{align*}


It is clear that $Q_{(2)}$ or $Q_{(m)}$ would still be a convex combination of
elements in $\mathcal{Q}$.

Another way to look at Li's is that we have. It remains unclear how the 
minimization is actually being done in practice.

### Convexity of the KL

KL is convex w.r.t. *distributions* in $\mathcal{Q}$. However, it is not convex
w.r.t. the parameter space.

TODO: show that it is convex in 2x2 contingency table case but not exponetial 
family cases.

### Requirement 

To quote Brinda, Li's inequality requires the family $\mathcal{Q}$ to have a 
uniformly bounded density ratio.


## Cisszar Algorithm

Originally proposed by @csiszarInformationGeometryAlternating1984.